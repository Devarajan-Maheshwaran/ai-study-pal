{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2baf42",
   "metadata": {},
   "source": [
    "# AI Study Pal Capstone: Summarizer Model Training\n",
    "This notebook trains a simple Keras neural network to summarize text for the AI Study Pal project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38915309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca786f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load and Explore Synthetic Dataset\n",
    "# Each sample: long text, short summary\n",
    "texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables systems to learn from data and improve over time without being explicitly programmed.\",\n",
    "    \"Supervised learning uses labeled data to train models to predict outcomes for new, unseen data.\",\n",
    "    \"Unsupervised learning finds patterns and relationships in data without labeled outcomes.\",\n",
    "    \"Neural networks are computational models inspired by the human brain, used for tasks like classification and regression.\",\n",
    "    \"Logistic regression is a statistical method for binary classification problems.\",\n",
    "]\n",
    "summaries = [\n",
    "    \"Machine learning lets systems learn from data.\",\n",
    "    \"Supervised learning uses labeled data for prediction.\",\n",
    "    \"Unsupervised learning finds patterns in unlabeled data.\",\n",
    "    \"Neural networks mimic the brain for classification.\",\n",
    "    \"Logistic regression solves binary classification.\"\n",
    "]\n",
    "df = pd.DataFrame({'text': texts, 'summary': summaries})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Preprocessing\n",
    "vectorizer = CountVectorizer(max_features=50)\n",
    "X = vectorizer.fit_transform(df['text']).toarray()\n",
    "Y = vectorizer.transform(df['summary']).toarray()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Engineering\n",
    "# (BoW features used; can add text length)\n",
    "df['length'] = df['text'].apply(len)\n",
    "X_len = np.array(df['length']).reshape(-1, 1)\n",
    "X_full = np.hstack([X, X_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Selection and Training\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_full.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(Y.shape[1], activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=Adam(0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_full, Y, epochs=100, batch_size=2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4f5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model Evaluation\n",
    "loss, acc = model.evaluate(X_full, Y, verbose=0)\n",
    "print(f\"Train Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22355bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Hyperparameter Tuning\n",
    "# (Demo: Try different learning rates)\n",
    "for lr in [0.001, 0.01, 0.1]:\n",
    "    model.compile(optimizer=Adam(lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_full, Y, epochs=20, batch_size=2, verbose=0)\n",
    "    loss, acc = model.evaluate(X_full, Y, verbose=0)\n",
    "    print(f\"LR={lr}: Train Accuracy={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Model Deployment Example\n",
    "# Save model and vectorizer\n",
    "model.save('summarizer_model.h5')\n",
    "joblib.dump(vectorizer, 'summarizer_vectorizer.pkl')\n",
    "\n",
    "# Load and use model\n",
    "from tensorflow.keras.models import load_model\n",
    "vectorizer_loaded = joblib.load('summarizer_vectorizer.pkl')\n",
    "model_loaded = load_model('summarizer_model.h5')\n",
    "\n",
    "sample_text = [\"Explain supervised learning in AI.\"]\n",
    "X_sample = vectorizer_loaded.transform(sample_text).toarray()\n",
    "X_sample_full = np.hstack([X_sample, np.array([len(t) for t in sample_text]).reshape(-1, 1)])\n",
    "pred_summary_vec = model_loaded.predict(X_sample_full)\n",
    "# Convert vector back to words (demo)\n",
    "words = vectorizer_loaded.get_feature_names_out()\n",
    "summary = ' '.join([words[i] for i, v in enumerate(pred_summary_vec[0]) if v > 0.1])\n",
    "print('Predicted summary:', summary)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
