{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1 – Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "print(\"All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 – Create training dataset for summarization\n",
    "data = [\n",
    "    {\n",
    "        \"input_text\": (\n",
    "            \"Machine learning is a field of artificial intelligence that \"\n",
    "            \"focuses on building systems that learn from data. These systems \"\n",
    "            \"can improve their performance on tasks over time without being \"\n",
    "            \"explicitly programmed for every rule.\"\n",
    "        ),\n",
    "        \"target_summary\": (\n",
    "            \"Machine learning is AI where systems learn from data and improve \"\n",
    "            \"performance over time.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": (\n",
    "            \"Python is a high-level, interpreted programming language known \"\n",
    "            \"for its readability and large ecosystem of libraries. It is \"\n",
    "            \"widely used in web development, data science, automation, and \"\n",
    "            \"machine learning.\"\n",
    "        ),\n",
    "        \"target_summary\": (\n",
    "            \"Python is a readable, high-level language used in web, data \"\n",
    "            \"science, automation, and ML.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": (\n",
    "            \"Supervised learning uses labeled data to train models, meaning \"\n",
    "            \"each input comes with the correct output. The model learns to \"\n",
    "            \"map inputs to outputs so it can make predictions on new, \"\n",
    "            \"unseen data.\"\n",
    "        ),\n",
    "        \"target_summary\": (\n",
    "            \"Supervised learning trains models on labeled data so they can \"\n",
    "            \"predict outputs for new inputs.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": (\n",
    "            \"Neural networks are computational models inspired by the human \"\n",
    "            \"brain. They consist of layers of interconnected nodes that can \"\n",
    "            \"learn complex patterns from data through training.\"\n",
    "        ),\n",
    "        \"target_summary\": (\n",
    "            \"Neural networks are layered models that learn complex patterns \"\n",
    "            \"from data, inspired by the brain.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": (\n",
    "            \"Deep learning is a subset of machine learning using neural networks \"\n",
    "            \"with multiple layers to learn representations of data. It has \"\n",
    "            \"achieved remarkable success in image recognition, natural language \"\n",
    "            \"processing, and other complex tasks.\"\n",
    "        ),\n",
    "        \"target_summary\": (\n",
    "            \"Deep learning uses multi-layer neural networks to learn data \"\n",
    "            \"representations, succeeding in vision and NLP tasks.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Created training dataset with {len(df)} samples\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3 – Prepare tokenizers\n",
    "input_texts = df[\"input_text\"].tolist()\n",
    "target_texts = df[\"target_summary\"].tolist()\n",
    "\n",
    "num_words = 5000\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "input_tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "\n",
    "target_tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "target_tokenizer.fit_on_texts(target_texts)\n",
    "\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "max_input_len = max(len(seq) for seq in input_sequences)\n",
    "max_target_len = max(len(seq) for seq in target_sequences)\n",
    "\n",
    "print(f\"Max input length: {max_input_len}\")\n",
    "print(f\"Max target length: {max_target_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e11d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4 – Pad sequences\n",
    "X = pad_sequences(input_sequences, maxlen=max_input_len, padding=\"post\", truncating=\"post\")\n",
    "y = pad_sequences(target_sequences, maxlen=max_target_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 – Build Keras embedding + LSTM model\n",
    "embedding_dim = 64\n",
    "latent_dim = 128\n",
    "vocab_size = num_words\n",
    "\n",
    "inputs = keras.Input(shape=(max_input_len,))\n",
    "x = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(latent_dim))(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "print(\"Model summary:\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb73744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6 – Prepare targets (first token of each summary)\n",
    "y_first_token = np.array([seq[0] if len(seq) > 0 else 0 for seq in target_sequences])\n",
    "print(f\"Target shape: {y_first_token.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011bc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 – Train Keras model\n",
    "history = model.fit(\n",
    "    X,\n",
    "    y_first_token,\n",
    "    epochs=50,\n",
    "    batch_size=2,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"\\nModel training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 – Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history[\"loss\"], label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Keras Model Training Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../data/summarizer_training.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Training plot saved to data/summarizer_training.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2550ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9 – Define extractive summarizer function\n",
    "def simple_extractive_summary(text: str, max_sentences: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Simple extractive summarizer: returns first N sentences.\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    if len(sentences) <= max_sentences:\n",
    "        return text\n",
    "\n",
    "    return \" \".join(sentences[:max_sentences])\n",
    "\n",
    "# Test it\n",
    "test_text = df[\"input_text\"].iloc[0]\n",
    "print(\"Original text:\")\n",
    "print(test_text)\n",
    "print(\"\\n\\nSummarized:\")\n",
    "print(simple_extractive_summary(test_text, max_sentences=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876fb9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10 – Save Keras model and tokenizers\n",
    "models_dir = \"../backend/models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(models_dir, \"summarizer_keras_model.h5\")\n",
    "input_tok_path = os.path.join(models_dir, \"summarizer_input_tokenizer.joblib\")\n",
    "target_tok_path = os.path.join(models_dir, \"summarizer_target_tokenizer.joblib\")\n",
    "\n",
    "model.save(model_path)\n",
    "joblib.dump(input_tokenizer, input_tok_path)\n",
    "joblib.dump(target_tokenizer, target_tok_path)\n",
    "\n",
    "print(f\"Keras model saved to: {model_path}\")\n",
    "print(f\"Input tokenizer saved to: {input_tok_path}\")\n",
    "print(f\"Target tokenizer saved to: {target_tok_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
