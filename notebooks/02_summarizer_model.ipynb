{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13461c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"input_text\": (\n",
    "            \"Machine learning is a field of artificial intelligence that \"\n",
    "            \"focuses on building systems that learn from data. These systems \"\n",
    "            \"can improve their performance on tasks over time without being \"\n",
    "            \"explicitly programmed for every rule.\"\n",
    "        ),\n",
    "        \"target_summary\": (\n",
    "            \"Machine learning is AI where systems learn from data and improve \"\n",
    "            \"performance over time.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": (\n",
    "            \"Python is a high-level, interpreted programming language known \"\n",
    "            \"for its readability and large ecosystem of libraries. It is \"\n",
    "            \"widely used in web development, data science, automation, and \"\n",
    "            \"machine learning.\"\n",
    "        ),\n",
    "        \"target_summary\": (\n",
    "            \"Python is a readable, high-level language used in web, data \"\n",
    "            \"science, automation, and ML.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": (\n",
    "            \"Supervised learning uses labeled data to train models, meaning \"\n",
    "            \"each input comes with the correct output. The model learns to \"\n",
    "            \"map inputs to outputs so it can make predictions on new, \"\n",
    "            \"unseen data.\"\n",
    "        ),\n",
    "        \"target_summary\": (\n",
    "            \"Supervised learning trains models on labeled data so they can \"\n",
    "            \"predict outputs for new inputs.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": (\n",
    "            \"Neural networks are computational models inspired by the human \"\n",
    "            \"brain. They consist of layers of interconnected nodes that can \"\n",
    "            \"learn complex patterns from data through training.\"\n",
    "        ),\n",
    "        \"target_summary\": (\n",
    "            \"Neural networks are layered models that learn complex patterns \"\n",
    "            \"from data, inspired by the brain.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26296b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df[\"input_text\"].tolist()\n",
    "target_texts = df[\"target_summary\"].tolist()\n",
    "\n",
    "num_words = 5000\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "input_tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "\n",
    "target_tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "target_tokenizer.fit_on_texts(target_texts)\n",
    "\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "max_input_len = max(len(seq) for seq in input_sequences)\n",
    "max_target_len = max(len(seq) for seq in target_sequences)\n",
    "\n",
    "max_input_len, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612af79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(input_sequences, maxlen=max_input_len, padding=\"post\", truncating=\"post\")\n",
    "y = pad_sequences(target_sequences, maxlen=max_target_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "latent_dim = 128\n",
    "vocab_size = num_words\n",
    "\n",
    "inputs = keras.Input(shape=(max_input_len,))\n",
    "x = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(latent_dim))(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)  # predict next-word distribution (simplified)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1df4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the first token of each target sequence as a simple classification target\n",
    "y_first_token = np.array([seq[0] if len(seq) > 0 else 0 for seq in target_sequences])\n",
    "\n",
    "X.shape, y_first_token.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ebad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X,\n",
    "    y_first_token,\n",
    "    epochs=50,\n",
    "    batch_size=2,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e79829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Download punkt once\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def simple_extractive_summary(text, max_sentences=2):\n",
    "    \"\"\"\n",
    "    Very simple extractive summarizer:\n",
    "    - Split into sentences\n",
    "    - If text is short, return as is\n",
    "    - Otherwise return the first `max_sentences` sentences\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    if len(sentences) <= max_sentences:\n",
    "        return text\n",
    "\n",
    "    summary = \" \".join(sentences[:max_sentences])\n",
    "    return summary\n",
    "\n",
    "# Quick test\n",
    "sample_text = df[\"input_text\"].iloc[0]\n",
    "print(\"Original:\")\n",
    "print(sample_text)\n",
    "print(\"\\nSummary:\")\n",
    "print(simple_extractive_summary(sample_text, max_sentences=2))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
